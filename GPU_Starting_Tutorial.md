## Topic: GPU+PyTorch+Triton Starting Tutorial

---

### 0. Hardware Information

> **Hardware**: ```NVIDIA A100```
>
> **Driver**: ```cuda_12.3.r12.3/compiler.33281558_0```

è¡¨ï¼š GPUç¡¬ä»¶çš„ä¿¡æ¯æ±‡æ€»

| æ¶æ„          | GPUå‹å·    | è®¡ç®—èƒ½åŠ› | `-arch`å‚æ•°   | å¤‡æ³¨               | CUDAç‰ˆæœ¬è¦æ±‚ |
| :------------ | :--------- | :------- | :------------ | :----------------- | ------------ |
| **Volta**     | V100       | 7.0      | `-arch=sm_70` | æ•°æ®ä¸­å¿ƒå¡         | 9.0+         |
| **Volta**     | Titan V    | 7.0      | `-arch=sm_70` | æ¶ˆè´¹çº§å¡           | 9.0+         |
| **Ampere**    | A100       | 8.0      | `-arch=sm_80` | æ•°æ®ä¸­å¿ƒå¡         | 11.0+        |
| **Ampere**    | A30        | 8.0      | `-arch=sm_80` | æ•°æ®ä¸­å¿ƒå¡         | 11.0+        |
| **Ampere**    | RTX 30ç³»åˆ— | 8.6      | `-arch=sm_86` | æ¶ˆè´¹çº§å¡           | 11.0+        |
| **Hopper**    | H100       | 9.0      | `-arch=sm_90` | æ•°æ®ä¸­å¿ƒå¡         | 11.8+        |
| **Blackwell** | B200       | 9.0+     | `-arch=sm_90` | ç›®å‰ä½¿ç”¨Hopperå‚æ•° | 12.0+        |

<img src="./pictures/image-20251030145348766.png" alt="image-20251030145348766" style="zoom:50%;" />

(From NVIDIA CUDA Compiler Driver, same with PTX)

### 1. Building Environment

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ~/Miniconda3-latest-Linux-x86_64.sh
```

### 2. PyTorch + Triton + vLLM

```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

```bash
pip install triton==3.4.0
```

```bash
pip install vllm==0.9.2
```

**Verify the installations**: 

```python
# éªŒè¯è„šæœ¬ verify_installation.py
import torch
import triton
import vllm

print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"PyTorch CUDA å¯ç”¨: {torch.cuda.is_available()}")
print(f"PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}")
print(f"Triton ç‰ˆæœ¬: {triton.__version__}")
print(f"vLLM ç‰ˆæœ¬: {vllm.__version__}")

# æµ‹è¯•GPUè®¡ç®—
if torch.cuda.is_available():
    device = "cuda"
    x = torch.tensor([1.0, 2.0, 3.0]).to(device)
    y = x * 2
    print(f"GPUè®¡ç®—æµ‹è¯•æˆåŠŸ: {y}")
else:
    print("CUDAä¸å¯ç”¨ï¼")
```

### 3. CUDAç¼–ç¨‹åŸºç¡€

&emsp;CUDAç›¸å…³çš„ç¼–ç¨‹ç¼–è¯‘æµç¨‹å¯ä»¥æ€»ç»“ä¸ºå¦‚ä¸‹æµç¨‹å›¾ï¼š

```mermaid
flowchart TD
    A[CUDA C++ Source Code<br>.cu file] --> B[nvcc Compilation]
    
    subgraph B [nvcc Compilation Process]
        B1[Separate Host & Device Code]
        B2[Compile Device Code to PTX]
        B3[Compile PTX to cubin<br>Binary Code]
        B4[Embed PTX and/or cubin]
        B5[Compile Host Code]
    end
    
    B2 --> C[PTX Code<br>Virtual ISA<br>Platform Independent]
    B3 --> D[cubin Binary<br>GPU Specific]
    
    B5 --> E[Modified Host Code<br>with Runtime API Calls]
    
    C --> F[Runtime Loading & JIT Compilation]
    D --> F
    E --> F
    
    F --> G[Executable]
```

"Hello World"æ ·ä¾‹ä»£ç ï¼š

```c++
// add.cu
#include <iostream>
#include <math.h>
 
// Kernel function to add the elements of two arrays
__global__
void add(int n, float *x, float *y)
{
 for (int i = 0; i < n; i++)
   y[i] = x[i] + y[i];
}
 
int main(void)
{
 int N = 1<<20;
 float *x, *y;
 
 // Allocate Unified Memory â€“ accessible from CPU or GPU
 cudaMallocManaged(&x, N*sizeof(float));
 cudaMallocManaged(&y, N*sizeof(float));
 
 // initialize x and y arrays on the host
 for (int i = 0; i < N; i++) {
   x[i] = 1.0f;
   y[i] = 2.0f;
 }
 
 // Run kernel on 1M elements on the GPU
 add<<<1, 1>>>(N, x, y);
 
 // Wait for GPU to finish before accessing on host
 cudaDeviceSynchronize();
 
 // Check for errors (all values should be 3.0f)
 float maxError = 0.0f;
 for (int i = 0; i < N; i++) {
   maxError = fmax(maxError, fabs(y[i]-3.0f));
 }
 std::cout << "Max error: " << maxError << std::endl;
 
 // Free memory
 cudaFree(x);
 cudaFree(y);
  return 0;
}
```



#### (1) Kernel Function: è®¾å¤‡ä¾§å‡½æ•°ä¸»è¦ä»¥```__global__```å…³é”®è¯ä¿®é¥°

```c++
__global__ add(int n, float* x, float* y) {
  for (int i = 0; i < n; ++i) {
    y[i] = x[i] + y[i];
  }
}
```

#### (2) Unified Memory: host-deviceç»Ÿä¸€ç¼–å€çš„æ˜¾å­˜åˆ†é…ï¼Œåˆ©ç”¨```cudaMallocManaged()```åˆ†é…ç»Ÿä¸€å†…å­˜ï¼Œè¿”å›å¯è®¿é—®çš„æŒ‡é’ˆï¼Œ```kernel```æ‰§è¡Œå®Œæ¯•éœ€è¦æ‰‹åŠ¨åˆ©ç”¨```cudaFree()```é‡Šæ”¾å†…å­˜ï¼ˆ```cudaMallocManaged-cudaFree```å’Œæ ‡å‡†```C++```ä¸­çš„```new-delete```å¯¹åº”ï¼‰

```c++
// Allocate Unified Memory -- accessible from CPU or GPU
float *x, *y, *sum;
cudaMallocManaged(&x, N*sizeof(float));
cudaMallocManaged(&y, N*sizeof(float));
 
...
 
// Free memory
cudaFree(x);
cudaFree(y);
```

#### (3) Kernal Launch: ä»hostä¾§å¯åŠ¨Kernelå‡½æ•°ï¼Œä½¿ç”¨cudaçš„ä¸‰é‡è§’åº¦æ‹¬å·è¯­æ³•```<<<Dg, Db, Ns, S>>>```

> **æ³¨é‡Š**ï¼š
>
> (a) ```Dg```ä»£è¡¨æ•´ä¸ªgridçš„å°ºå¯¸ï¼ˆä¸€ä¸ªgridæœ‰å¤šå°‘ä¸ªblockï¼‰ï¼Œæ•°æ®ç±»å‹ä¸ºdim3ï¼Œä¾‹å¦‚ï¼šDim3 Dg(Dg.x, Dg.y, 1)ä»£è¡¨gridä¸­æ¯è¡Œæœ‰Dg.xä¸ªblockï¼Œæ¯åˆ—æœ‰Dg.yä¸ªblockï¼Œç¬¬ä¸‰ä¸ªçº¬åº¦æ’å®šä¸º1ï¼Œå› æ­¤æ•´ä¸ªgridä¸€å…±æœ‰```Dg.x * Dg.y```ä¸ªblockï¼Œä¸”$Dg.x, Dg.y <=  65535$;
>
> (b) Dbå®šä¹‰ä¸€ä¸ªblockçš„å°ºå¯¸ï¼ˆä¸€ä¸ªblockæœ‰å¤šå°‘threadï¼‰ï¼Œæ•°æ®ç±»å‹ä¸ºdim3ï¼Œä¾‹å¦‚ï¼šDim3 Db(Db.x, Db.y, Db.z)ä»£è¡¨blockä¸­æ¯è¡Œæœ‰Db.xä¸ªthreadï¼Œæ¯åˆ—æœ‰Db.yä¸ªthreadï¼Œé«˜åº¦æ–¹å‘æœ‰Db.zä¸ªthreadï¼Œå› æ­¤ä¸€ä¸ªblockæœ‰$Db.x * Db.y * Db.z$ä¸ªthread.
>
> (c) Nsä¸ºå¯é€‰å‚æ•°ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªblocké™¤äº†é™æ€åˆ†é…çš„å…±äº«å†…å­˜å¤–ï¼Œæœ€å¤šèƒ½åŠ¨æ€åˆ†é…çš„å…±äº«å†…å­˜å¤§å°ï¼Œå•ä½ä¸ºByteã€‚å¦‚æœä¸éœ€è¦ï¼Œåˆ™$Ns = 0$æˆ–è€…å‚æ•°ç¼ºçœ.
>
> (d) Sæ—¶cudaStream_tç±»å‹çš„å¯é€‰å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º$S = 0$ï¼Œæ ‡è¯†æ ¸å‡½æ•°ä½äºå“ªä¸ªstreamä¸­ï¼ˆæŒ‡å®šæ ¸å‡½æ•°åœ¨å“ªä¸ªstreamä¸­æ‰§è¡Œï¼‰

```c++
// ä»£è¡¨ä¸€ä¸ªblockï¼Œä¸€ä¸ªthread
add<<<1, 1>>>(N, sum, x, y);
```

#### (4) nvccç¼–è¯‘

> **ç¼–è¯‘é€‰é¡¹**ï¼š
>
> â€‹         ```--gpu-architecture```  :  æŒ‡å®švirtual architecture(PTX),  ä¾‹å¦‚ï¼šcompute_80, compute_90
>
> â€‹         ```--gpu-code```: æŒ‡å®šreal architecture, ä¾‹å¦‚ï¼šsm_80, sm_86

* åŸºç¡€ç¼–è¯‘å‘½ä»¤ï¼šç›´æ¥å¾—åˆ°å¯æ‰§è¡Œæ–‡ä»¶

```bash
nvcc -o add add.cu
```

* æŒ‡å®šå¹³å°ï¼šä»V100, A100, H100åˆ°B200ä¸åŒGPUå‹å·ï¼Œnvccå¯ä»¥ç¼–è¯‘å¾—åˆ°ç‰¹å®šå¹³å°äºŒè¿›åˆ¶ã€ptxæŒ‡ä»¤æˆ–è€…é€šç”¨å¹³å°æŒ‡ä»¤ï¼ˆå°±æ˜¯å„ä¸ªå¹³å°çš„æŒ‡ä»¤æ‰“åŒ…ï¼‰

```bash
# æŸ¥è¯¢å½“å‰ä½¿ç”¨GPUå¹³å°çš„å¡å‹å·ç­‰ä¿¡æ¯
nvidia-smi
# æŸ¥è¯¢å½“å‰ä½¿ç”¨çš„GPUå¹³å°çš„è®¡ç®—èƒ½åŠ›
nvidia-smi --query-gpu=compute_cap --format=csv
```

```bash
# ç¼–è¯‘åˆ°ç‰¹å®šå¹³å°

## (1) V100åŠæ‰€æœ‰è®¡ç®—èƒ½åŠ›7.0çš„GPU
nvcc -arch=sm_70 -o your_program your_program.cu
# å¦‚æœéœ€è¦åŒ…å«PTXä»£ç ä»¥æ”¯æŒæœªæ¥å…¼å®¹æ€§
nvcc -arch=sm_70 -gencode arch=compute_70,code=sm_70 -o your_program your_program.cu
## (2) A100åŠæ‰€æœ‰è®¡ç®—èƒ½åŠ›8.0çš„GPU
nvcc -arch=sm_80 -o your_program your_program.cu
# å¦‚æœéœ€è¦åŒ…å«PTXä»£ç ä»¥æ”¯æŒæœªæ¥å…¼å®¹æ€§
nvcc -arch=sm_80 -gencode arch=compute_80,code=sm_80 -o your_program your_program.cu
## (3) H100åŠæ‰€æœ‰è®¡ç®—èƒ½åŠ›9.0çš„GPU
nvcc -arch=sm_90 -o your_program your_program.cu
# å¦‚æœéœ€è¦åŒ…å«PTXä»£ç ä»¥æ”¯æŒæœªæ¥å…¼å®¹æ€§
nvcc -arch=sm_90 -gencode arch=compute_90,code=sm_90 -o your_program your_program.cu
## (4) B200åŠæ‰€æœ‰è®¡ç®—èƒ½åŠ›9.0çš„GPU
nvcc -arch=sm_90 -o your_program your_program.cu
# å¦‚æœéœ€è¦åŒ…å«PTXä»£ç ä»¥æ”¯æŒæœªæ¥å…¼å®¹æ€§
nvcc -arch=sm_90 -gencode arch=compute_90,code=sm_90 -o your_program your_program.cu
```

```bash
# ç¼–è¯‘åˆ°é€šç”¨å¹³å°
# æ”¯æŒV100åˆ°H100çš„é€šç”¨äºŒè¿›åˆ¶
nvcc -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -o universal_program program.cu
```

å½“å‰ä½¿ç”¨çš„GPUç¡¬ä»¶ä¸ºA100ï¼Œå› æ­¤ä»¥ä¸Š```add.cu```ä»£è¡¨ç¼–è¯‘å‘½ä»¤ä¸ºï¼š

```bash
nvcc -arch=sm_80 -o add add.cu
```

æ‰§è¡Œåå¾—åˆ°ç»“æœï¼š

```bash
(pytorch) lthpc@gnode02:~/chengl/Programming> ./add
Max error: 0
```

* CUDAç¼–è¯‘åŸºæœ¬è®¾è®¡åŸç†

&emsp;ä¸ºäº†å°½å¯èƒ½æå‡CUDAä»£ç çš„è·¨å¹³å°å…¼å®¹æ€§ï¼ŒCUDAçš„ç¼–è¯‘è¿‡ç¨‹åˆ†åˆ«é’ˆå¯¹ä¸¤çº§æ¶æ„ï¼š```virtual intermediate architecture(VIA)```å’Œ```real GPU architecture(RGA)```. ä¸¤çº§æ¶æ„çš„ä¸­é—´è¡¨ç¤ºä¸º```PTX```ï¼Œ```PTX```å¯ä»¥çœ‹åš```VIA```çš„```Assembly code```å’Œ```RGA```çš„æºä»£ç ï¼Œ```PTX```çš„é€‰æ‹©åº”è¯¥ä½¿```VIA```å°½å¯èƒ½```low-level```ï¼Œè€Œ```RGA```å°½å¯èƒ½çš„```high-level```. å¦‚æœéœ€è¦å°½å¯èƒ½æé«˜åº”ç”¨ä»£ç çš„å¯ç§»æ¤æ€§(ä¸ç¡®å®šGPUçš„å¹³å°)ï¼Œå¯ä»¥é‡‡ç”¨```just-in-time```ç¼–è¯‘æ–¹å¼ï¼Œä½†æ˜¯```JIT```ä¸€ä¸ªç¼ºç‚¹æ˜¯ç¨‹åº``startup delay``è¿‡é•¿ï¼Œè§£å†³è¯¥é—®é¢˜çš„ä¸¤ä¸ªæ–¹æ³•åˆ†åˆ«æ˜¯ï¼š`compilation cache`å’Œ`Fatbinaries`ã€‚

```shell
# JIT compilation
nvcc x.cu --gpu-architecture=compute_90 --gpu-code=compute_90
```

```shell
# Fatbinaries: This command generates exact code for two architectures, plus PTX code for use by JIT in case a next generation GPU is encountered.
nvcc x.cu --gpu-architecture=compute_80 --gpu-code=compute_80,sm_86,sm_89
```

&emsp;ä¸‹é¢ç»™å‡ºäº†CUDAä»£ç å¸¸è§„ä¸¤çº§ç¼–è¯‘æµç¨‹å’Œ```JIT```ç¼–è¯‘æµç¨‹ç¤ºæ„å›¾ã€‚

ï¼ˆ1ï¼‰CUDAä»£ç å¸¸è§„ä¸¤çº§ç¼–è¯‘æµç¨‹

```mermaid
graph TD
    A[æºä»£ç ] --> B[é¢„å¤„ç†]
    B --> C[ç¼–è¯‘ä¸ºPTXä¸­é—´ä»£ç ]
    C --> D{ç¼–è¯‘æ¨¡å¼}
    D -->|æ•´ä½“ç¨‹åºç¼–è¯‘| E[ç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶]
    D -->|å•ç‹¬ç¼–è¯‘| F[æ±‡ç¼–ä¸ºSASS]
    F --> G[é“¾æ¥æˆæœ€ç»ˆäºŒè¿›åˆ¶æ–‡ä»¶]
```

ï¼ˆ2ï¼‰JITç¼–è¯‘æµç¨‹

```mermaid
graph TD
    A[æºä»£ç ] --> B[ä»£ç è§£æ]
    B --> C{ä¸­é—´è¡¨ç¤º}
    C -->|LLVM IR/PTX| D[åŠ¨æ€ä¼˜åŒ–]
    D --> E[æœºå™¨ç ç”Ÿæˆ]
    E --> F[GPUæ‰§è¡Œ]
    F --> G[ç»“æœè¾“å‡º]

    subgraph ç¼–è¯‘æµç¨‹
        C -->|PTXä¸­é—´ä»£ç | H[ç¼–è¯‘å™¨]
        H --> I[ä¼˜åŒ–å™¨]
        I --> J[ä»£ç ç”Ÿæˆå™¨]
        J --> K[ç›®æ ‡æ¶æ„æœºå™¨ç ]
    end

    subgraph æ‰§è¡Œé˜¶æ®µ
        K --> L[GPUæ‰§è¡Œå¼•æ“]
        L --> M[çº¿ç¨‹è°ƒåº¦]
        M --> N[è®¡ç®—ç»“æœ]
    end
```



#### (5) cudaåŸç”Ÿæ€§èƒ½Profilingé‡‡é›†å·¥å…·: ```nsys```

* ç›´æ¥é‡‡é›†å®Œæ•´æ€§èƒ½ç»Ÿè®¡æ•°æ®

```bash
nsys profile -t cuda --stats=true ./add
```

ç»“æœä¼šç”Ÿæˆå¦‚ä¸‹è¯¦ç»†æ€§èƒ½ä¿¡æ¯ï¼šåŒ…æ‹¬å†…å­˜åˆ†é…ï¼ˆ```cudaMallocManaged```ï¼‰ï¼ŒåŒæ­¥ï¼Œå†…å­˜é‡Šæ”¾ï¼ŒKernelå¯åŠ¨æ—¶é—´ï¼ŒKernelæ‰§è¡Œæ—¶é—´ç­‰

```bash
(pytorch) lthpc@gnode02:~/chengl/Programming> nsys profile -t cuda --stats=true ./add
Max error: 0
Generating '/tmp/nsys-report-94b0.qdstrm'
[1/6] [========================100%] report2.nsys-rep
[2/6] [========================100%] report2.sqlite
[3/6] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls    Avg (ns)       Med (ns)      Min (ns)     Max (ns)     StdDev (ns)            Name         
 --------  ---------------  ---------  -------------  -------------  -----------  -----------  -------------  ----------------------
     64.2      308,780,682          2  154,390,341.0  154,390,341.0       19,020  308,761,662  218,314,015.8  cudaMallocManaged     
     35.7      171,461,980          1  171,461,980.0  171,461,980.0  171,461,980  171,461,980            0.0  cudaDeviceSynchronize 
      0.1          468,500          2      234,250.0      234,250.0      183,820      284,680       71,318.8  cudaFree              
      0.0          197,110          1      197,110.0      197,110.0      197,110      197,110            0.0  cudaLaunchKernel      
      0.0              840          1          840.0          840.0          840          840            0.0  cuModuleGetLoadingMode

[4/6] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)             Name           
 --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  --------------------------
    100.0      171,458,500          1  171,458,500.0  171,458,500.0  171,458,500  171,458,500          0.0  add(int, float *, float *)

[5/6] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)               Operation              
 --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------------
     61.3          978,906     48  20,393.9   6,575.5     3,679   105,411     29,701.5  [CUDA memcpy Unified Host-to-Device]
     38.7          617,794     24  25,741.4   5,167.5     2,655   151,107     43,848.0  [CUDA memcpy Unified Device-to-Host]

[6/6] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)               Operation              
 ----------  -----  --------  --------  --------  --------  -----------  ------------------------------------
      8.389     48     0.175     0.033     0.004     1.044        0.304  [CUDA memcpy Unified Host-to-Device]
      4.194     24     0.175     0.033     0.004     1.044        0.307  [CUDA memcpy Unified Device-to-Host]

Generated:
    /home/lthpc/chengl/Programming/report2.nsys-rep
    /home/lthpc/chengl/Programming/report2.sqlite
```

* ç®€åŒ–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ï¼š[Mark Harris](https://developer.nvidia.com/zh-cn/blog/author/mark-harris/) å†™äº†ä¸€ä¸ªç®€å•çš„nsys_easyè„šæœ¬(https://github.com/harrism/nsys_easy)ï¼Œç”¨æ¥ç®€åŒ–è¾“å‡ºä¿¡æ¯

```bash
# ä¿®æ”¹æ‰§è¡Œæƒé™
chmod 755 ~/nsys_easy/nsys_easy
# æ·»åŠ nsys_easyç¯å¢ƒå˜é‡
export PATH=~/nsys_easy:$PATH
# é‡‡é›†æ€§èƒ½ä¿¡æ¯
nsys_easy ./add
```

ç»“æœä¼šç”Ÿæˆå¦‚ä¸‹ç®€åŒ–ä¿¡æ¯ï¼š
```bash
(pytorch) lthpc@gnode02:~/chengl/Programming> nsys_easy ./add
Max error: 0
Generating '/tmp/nsys-report-66ad.qdstrm'
[1/1] [========================100%] nsys_easy.nsys-rep
Generated:
    /home/lthpc/chengl/Programming/nsys_easy.nsys-rep
Generating SQLite file nsys_easy.sqlite from nsys_easy.nsys-rep
Processing 939 events: [===================================================100%]
Processing [nsys_easy.sqlite] with [/home/software/cuda-12.3/nsight-systems-2023.3.3/host-linux-x64/reports/cuda_gpu_sum.py]... 

 ** CUDA GPU Summary (Kernels/MemOps) (cuda_gpu_sum):

 Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)   Category                 Operation              
 --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  -----------  ------------------------------------
     99.1      172,191,985          1  172,191,985.0  172,191,985.0  172,191,985  172,191,985          0.0  CUDA_KERNEL  add(int, float *, float *)          
      0.6          979,518         48       20,406.6        6,527.5        3,647      105,666     29,680.1  MEMORY_OPER  [CUDA memcpy Unified Host-to-Device]
      0.4          618,691         24       25,778.8        5,199.5        2,495      151,235     43,882.6  MEMORY_OPER  [CUDA memcpy Unified Device-to-Host]
```

#### (6) PyTorch + CUDAåŸç”Ÿé›†æˆå¼€å‘ + pyTorchç»Ÿä¸€æ€§èƒ½Profilingé‡‡é›†å·¥å…·ï¼š```torch.profiler```

&emsp;å‚è€ƒPyTorchå®˜ç½‘è¯´æ˜ï¼šhttps://docs.pytorch.org/docs/stable/profiler.html

&emsp;ç›®å‰LLMåŸºæœ¬éƒ½æ˜¯åœ¨PyTorchçš„pythonicç¯å¢ƒä¸‹å¼€å‘ä½¿ç”¨ï¼Œä¸ºäº†å®ç°CUDAå‡½æ•°åœ¨PyTorchçš„è‡ªç„¶é›†æˆï¼Œéœ€è¦ç”¨åˆ°```torch.utils.cpp_extension```å’Œ```torch/extension.h```ï¼Œå¹¶åˆ©ç”¨```pybind11```å®Œæˆpytorchä¸‹å¯¹CUDA-C/C++å‡½æ•°çš„è°ƒç”¨ã€‚https://github.com/gau-nernst/learn-cudaä¸­äº†ç»™äº†10ä¸ªç¤ºä¾‹æ•™ç¨‹ï¼Œè¿™é‡Œç»™å‡º```01-vector addition```çš„ä»£ç ç¤ºä¾‹ã€‚

<img src="./pictures/image-20251026145512062.png" alt="image-20251026145512062" style="zoom:40%;" />

```c++
// add.cu
#include <torch/extension.h>

#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_INPUT(x)                                                                                                 \
  CHECK_CUDA(x);                                                                                                       \
  CHECK_CONTIGUOUS(x)

__global__ void add_kernel(const float *input1, const float *input2, float *output, int size) {
  const int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < size)
    output[idx] = input1[idx] + input2[idx];
}

torch::Tensor add(torch::Tensor input1, torch::Tensor input2) {
  CHECK_INPUT(input1);
  CHECK_INPUT(input2);
  int size = input1.numel();
  TORCH_CHECK(size == input2.numel(), "input1 and input2 must have the same size");
  torch::Tensor output = torch::empty(size, input1.options());

  int n_threads = 256;
  int n_blocks = (size + n_threads - 1) / n_threads;
  add_kernel<<<n_blocks, n_threads>>>(input1.data_ptr<float>(), input2.data_ptr<float>(), output.data_ptr<float>(), size);

  return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def("add", &add, "Add two vectors"); }
```

&emsp;å®é™…ä¸Šå°±æ˜¯æ·»åŠ äº†ä¸€ä¸ªtensorçš„wrapperï¼Œå› ä¸ºtensoræ˜¯pytorchçš„åŸºç¡€æ•°æ®ç±»å‹ï¼Œå› æ­¤æ ¸å¿ƒæ­¥éª¤å°±æ˜¯å°†åŸå§‹cudaçš„å…¥å‚å’Œè¿”å›å‚æ•°ç­‰è½¬æˆtensorï¼ˆæŒ‡é’ˆï¼‰ï¼Œä¼ ç»™torchå®ç°åŒ…è£…ã€‚ä¸‹é¢æ˜¯pytorchä¸­çš„è°ƒç”¨ä»£ç ï¼ˆå¯ä»¥çœ‹åˆ°å°±æ˜¯é‡æ–°å£°æ˜äº†cudaå‡½æ•°ï¼Œå°†å…¶å½’ç±»ä¸ºä¸€ç±»moduleï¼Œä»è€Œä¸‹é¢å¯ä»¥ç›´æ¥ä»moduleä¸­è°ƒç”¨åŸå§‹å°è£…åçš„cudaå‡½æ•°ï¼Œå¦‚æœéœ€è¦å°†å‡½æ•°æ³¨å†Œè¿›torchï¼Œå³torch.addï¼Œåç»­ä¼šæ¶‰åŠï¼‰ï¼š

```python
# main.py
import torch
import torch.utils.cpp_extension
from torch.profiler import profile, record_function, ProfilerActivity
import matplotlib.pyplot as plt

module = torch.utils.cpp_extension.load(
    "module",
    sources=["add.cu"],
    extra_cuda_cflags=["-O3"],
    verbose=True,
)

# Example usage
input1 = torch.randn(1024000, device="cuda")
input2 = torch.randn(1024000, device="cuda")
output = module.add(input1, input2)
```

&emsp;ä¸‹é¢ç»™å‡ºtorch.profileré‡‡é›†ä»¥ä¸Šå°è£…åçš„cudaå‡½æ•°æ–¹æ³•ã€‚

* **(a) æœ€åŸºç¡€çš„```torch.profiler```è£¸æµ‹è¯•è°ƒç”¨é‡‡é›†æ€§èƒ½æ•°æ®**

```python
# æ ¸å¿ƒç»“æ„
from torch.profiler import profile, record_function, ProfilerActivity
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    code_to_profile() # æ›¿æ¢æˆéœ€è¦é‡‡é›†çš„torchä»£ç ï¼ˆå‡½æ•°ï¼‰
print(p.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1))
```

å®Œæ•´æµ‹è¯•ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.utils.cpp_extension
from torch.profiler import profile, record_function, ProfilerActivity
import matplotlib.pyplot as plt

module = torch.utils.cpp_extension.load(
    "module",
    sources=["add.cu"],
    extra_cuda_cflags=["-O3"],
    verbose=True,
)

# Example usage
input1 = torch.randn(1024000, device="cuda")
input2 = torch.randn(1024000, device="cuda")

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    output = module.add(input1, input2)
print(p.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1))
p.export_chrome_trace("trace.json")  # å¯ä»¥ç”Ÿæˆç¨‹åºè¿è¡Œçš„timeline
```

è¿è¡Œåçš„ç»“æœå¦‚ä¸‹ï¼š

```bash
(pytorch) lthpc@gnode02:~/chengl/Programming/learn-cuda/1_vector_addition> CUDA_VISIBLE_DEVICES=0 python main.py 
Using /home/lthpc/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/lthpc/.cache/torch_extensions/py310_cu126/module/build.ninja...
Building extension module module...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module module...
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
    add_kernel(float const*, float const*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us       9.824us       100.00%       9.824us       9.824us             1  
                                            aten::empty         5.96%      52.520us        75.63%     666.350us     666.350us       0.000us         0.00%       0.000us       0.000us             1  
                                           Unrecognized        69.67%     613.830us        69.67%     613.830us     613.830us       0.000us         0.00%       0.000us       0.000us             1  
                                       cudaLaunchKernel         8.46%      74.530us        23.81%     209.810us     209.810us       0.000us         0.00%       0.000us       0.000us             1  
                       Runtime Triggered Module Loading        15.35%     135.280us        15.35%     135.280us      67.640us       0.000us         0.00%       0.000us       0.000us             2  
                                  cudaDeviceSynchronize         0.56%       4.910us         0.56%       4.910us       4.910us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 881.070us
Self CUDA time total: 9.824us
```

&emsp;ä»ä¸Šé¢åˆ—è¡¨ä¸­æ‰¾åˆ°```Name = add_kernel```å¯¹åº”çš„```Self CUDA= 9.824us```å³ä¸ºGPUä¸Šadd_kernelæ‰§è¡Œçš„å®Œæ•´æ—¶é—´ï¼Œä¸‹é¢è¿˜å¯ä»¥å‘ç°```cudaLaunchKernel```ä»£è¡¨ä»hostä¾§å¯åŠ¨GPUä¾§æ ¸å‡½æ•°èŠ±è´¹æ—¶é—´ä¸º```209.810us```ï¼Œå¯ä»¥çœ‹åˆ°è¿œè¿œé•¿äºKernelæœ¬èº«åœ¨GPUä¸Šæ‰§è¡Œæ—¶é—´ã€‚

* **(b) è€ƒè™‘é¢„çƒ­ç­‰å› ç´ åçš„```torch.profiler```è°ƒç”¨æ€§èƒ½é‡‡é›†æ–¹æ³•**

```python
# æ ¸å¿ƒç»“æ„
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    code_to_profile_0()
    // turn off collection of all CUDA activity
    p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CUDA])
    code_to_profile_1()
    // turn on collection of all CUDA activity
    p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA])
    code_to_profile_2()
print(p.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1))
```

å®Œæ•´æµ‹è¯•ä»£ç ï¼š

```python
import torch
import torch.utils.cpp_extension
from torch.profiler import profile, record_function, ProfilerActivity
import matplotlib.pyplot as plt

module = torch.utils.cpp_extension.load(
    "module",
    sources=["add.cu"],
    extra_cuda_cflags=["-O3"],
    verbose=True,
)

# Example usage
input1 = torch.randn(1024000, device="cuda")
input2 = torch.randn(1024000, device="cuda")

# Non-default profiler schedule allows user to turn profiler on and off
# on different iterations of the training loop;
# trace_handler is called every time a new trace becomes available
def trace_handler(prof):
    print(
        prof.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1)
    )
    prof.export_chrome_trace("/tmp/test_trace_" + str(prof.step_num) + ".json")

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    # In this example with wait=1, warmup=1, active=2, repeat=1,
    # profiler will skip the first step/iteration,
    # start warming up on the second, record
    # the third and the forth iterations,
    # after which the trace will become available
    # and on_trace_ready (when set) is called;
    # the cycle repeats starting with the next step
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=7, repeat=1),
    on_trace_ready=trace_handler,
    record_shapes=True,
    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')
    # used when outputting for tensorboard
) as p:
    for iter in range(10):
        output = module.add(input1, input2)
        # send a signal to the profiler that the next iteration has started
        p.step()
```

é‡‡é›†ç»“æœå¦‚ä¸‹ï¼š

```shell
(pytorch) lthpc@gnode02:~/chengl/Programming/learn-cuda/1_vector_addition> CUDA_VISIBLE_DEVICES=0 python main.py 
Using /home/lthpc/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/lthpc/.cache/torch_extensions/py310_cu126/module/build.ninja...
Building extension module module...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module module...
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
    add_kernel(float const*, float const*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us      45.956us       100.00%      45.956us       6.565us             7  
                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us      45.956us       100.00%      45.956us       6.565us             7  
                                          ProfilerStep*        71.61%     380.100us        96.98%     514.710us      73.530us       0.000us         0.00%       0.000us       0.000us             7  
                                            aten::empty        12.90%      68.450us        12.90%      68.450us       9.779us       0.000us         0.00%       0.000us       0.000us             7  
                                       cudaLaunchKernel        12.47%      66.160us        12.47%      66.160us       9.451us       0.000us         0.00%       0.000us       0.000us             7  
                                  cudaDeviceSynchronize         3.02%      16.050us         3.02%      16.050us      16.050us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 530.760us
Self CUDA time total: 45.956us
```

&emsp;å¯ä»¥çœ‹åˆ°è€ƒè™‘é¢„çƒ­å’Œå¤šæ¬¡å¹³å‡åçš„```add_kernel```æ—¶é—´ä¸º```6.565us```ï¼Œå¹¶ä¸”å¤šæ¬¡è¿è¡Œæ—¶é—´å·®åˆ«ä¹Ÿä¸å¤§ã€‚

**(c) è€ƒè™‘é¢„çƒ­ç­‰å› ç´  + ä»…é‡‡é›†éƒ¨åˆ†ä»£ç çš„```torch.profiler```è°ƒç”¨æ€§èƒ½é‡‡é›†æ–¹æ³•ï¼š```toggle_collection_dynamic```**

```python
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    code_to_profile_0()
    // turn off collection of all CUDA activity
    p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CUDA])
    code_to_profile_1()
    // turn on collection of all CUDA activity
    p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA])
    code_to_profile_2()
print(p.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1))
```

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.utils.cpp_extension
from torch.profiler import profile, record_function, ProfilerActivity
import matplotlib.pyplot as plt

module = torch.utils.cpp_extension.load(
    "module",
    sources=["add.cu"],
    extra_cuda_cflags=["-O3"],
    verbose=True,
)

# Non-default profiler schedule allows user to turn profiler on and off
# on different iterations of the training loop;
# trace_handler is called every time a new trace becomes available
def trace_handler(prof):
    print(
        prof.key_averages().table(sort_by="self_cuda_time_total", row_limit=-1)
    )
    prof.export_chrome_trace("/tmp/test_trace_" + str(prof.step_num) + ".json")

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    # In this example with wait=1, warmup=1, active=2, repeat=1,
    # profiler will skip the first step/iteration,
    # start warming up on the second, record
    # the third and the forth iterations,
    # after which the trace will become available
    # and on_trace_ready (when set) is called;
    # the cycle repeats starting with the next step
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=7, repeat=1),
    on_trace_ready=trace_handler,
    record_shapes=True,
    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')
    # used when outputting for tensorboard
) as p:
    for iter in range(10):
        p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA])  # åŒæ—¶å…³æ‰CPUå’ŒGPUä¸Šçš„æ€§èƒ½é‡‡é›†
        input1 = torch.randn(1024000, device="cuda")  # ä¸ºäº†æµ‹è¯•æ•ˆæœï¼Œå°†æ•°æ®ç”Ÿæˆä¹Ÿè€ƒè™‘å¦‚ä¸‹é‡‡é›†
        input2 = torch.randn(1024000, device="cuda")
        p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA]) # ä»…ä»…æ‰“å¼€GPUä¸Šçš„æ€§èƒ½é‡‡é›†
        output = module.add(input1, input2)
        # send a signal to the p
        p.step()
```

profilingç»“æœå¦‚ä¸‹ï¼š

<img src="./pictures/image-20251026160214958.png" alt="image-20251026160214958" style="zoom:50%;" />

**ä½œä¸ºå¯¹æ¯”**ï¼š

(a) ç±»å‹ä¸€

```python
for iter in range(10):
        p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA])  # åŒæ—¶å…³æ‰CPUå’ŒGPUä¸Šçš„æ€§èƒ½é‡‡é›†
        input1 = torch.randn(1024000, device="cuda")  # ä¸ºäº†æµ‹è¯•æ•ˆæœï¼Œå°†æ•°æ®ç”Ÿæˆä¹Ÿè€ƒè™‘å¦‚ä¸‹é‡‡é›†
        input2 = torch.randn(1024000, device="cuda")
        output = module.add(input1, input2)
        # send a signal to the p
        p.step()
```

<img src="./pictures/image-20251026162830112.png" alt="image-20251026162830112" style="zoom:50%;" />

(b) ç±»å‹äºŒ

```python
for iter in range(10):
        p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA])  # åŒæ—¶å…³æ‰CPUå’ŒGPUä¸Šçš„æ€§èƒ½é‡‡é›†
        input1 = torch.randn(1024000, device="cuda")  # ä¸ºäº†æµ‹è¯•æ•ˆæœï¼Œå°†æ•°æ®ç”Ÿæˆä¹Ÿè€ƒè™‘å¦‚ä¸‹é‡‡é›†
        input2 = torch.randn(1024000, device="cuda")
        p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CPU]) # ä»…ä»…æ‰“å¼€CPUä¸Šçš„æ€§èƒ½é‡‡é›†
        output = module.add(input1, input2)
        # send a signal to the p
        p.step()
```

<img src="./pictures/image-20251026162947196.png" alt="image-20251026162947196" style="zoom:50%;" />

(c) ç±»å‹ä¸‰

```python
for iter in range(10):
        p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA])  # åŒæ—¶å…³æ‰CPUå’ŒGPUä¸Šçš„æ€§èƒ½é‡‡é›†
        input1 = torch.randn(1024000, device="cuda")  # ä¸ºäº†æµ‹è¯•æ•ˆæœï¼Œå°†æ•°æ®ç”Ÿæˆä¹Ÿè€ƒè™‘å¦‚ä¸‹é‡‡é›†
        input2 = torch.randn(1024000, device="cuda")
        p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]) # æ‰“å¼€CPU+GPUä¸Šçš„æ€§èƒ½é‡‡é›†
        output = module.add(input1, input2)
        # send a signal to the p
        p.step()
```

<img src="./pictures/image-20251026163203772.png" alt="image-20251026163203772" style="zoom:50%;" />

(d) ç±»å‹å››

```python
for iter in range(10):
        p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA])  # åŒæ—¶å…³æ‰CPUå’ŒGPUä¸Šçš„æ€§èƒ½é‡‡é›†
        input1 = torch.randn(1024000, device="cuda")  # ä¸ºäº†æµ‹è¯•æ•ˆæœï¼Œå°†æ•°æ®ç”Ÿæˆä¹Ÿè€ƒè™‘å¦‚ä¸‹é‡‡é›†
        input2 = torch.randn(1024000, device="cuda")
        p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]) # æ‰“å¼€CPU+GPUä¸Šçš„æ€§èƒ½é‡‡é›†
        output = module.add(input1, input2)
        # send a signal to the p
        p.step()
```

<img src="./pictures/image-20251026163340731.png" alt="image-20251026163340731" style="zoom:50%;" />

#### (7) CUDA KernelåŸºæœ¬ä¼˜åŒ–æ–¹æ³•









### 4. TensorRT-LLM

åœ¨NVIDIA A100 GPUä¸Šéƒ¨ç½²TensorRT-LLMèƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ä¸‹é¢æˆ‘å°†ä¸ºä½ æ¢³ç†è¯¦ç»†çš„å®‰è£…æ­¥éª¤ã€æ¨¡å‹éƒ¨ç½²æµç¨‹ä»¥åŠæ€§èƒ½æµ‹è¯•æ–¹æ³•ã€‚

### ğŸ› ï¸ TensorRT-LLM å®‰è£…æŒ‡å—

TensorRT-LLMçš„å®‰è£…ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼ï¼Œä½ å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

| å®‰è£…æ–¹å¼     | é€‚ç”¨åœºæ™¯               | è¯´æ˜                                       |
| :----------- | :--------------------- | :----------------------------------------- |
| **PIP å®‰è£…** | å¿«é€Ÿå¼€å§‹ï¼Œæ— éœ€å¤æ‚é…ç½® | ä¸€æ¡å‘½ä»¤å³å¯å®Œæˆï¼Œé€‚åˆä½“éªŒå’Œå¿«é€ŸåŸå‹éªŒè¯ã€‚ |
| **NGC å®¹å™¨** | ä¿è¯ç¯å¢ƒä¸€è‡´æ€§å’Œéš”ç¦»æ€§ | æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œé¿å…äº†ä¾èµ–å†²çªã€‚         |
| **æºç ç¼–è¯‘** | éœ€è¦æœ€æ–°ç‰¹æ€§æˆ–ç‰¹å®šå®šåˆ¶ | è¿‡ç¨‹æœ€å¤æ‚ï¼Œä½†èƒ½è·å–æœ€å‰æ²¿çš„åŠŸèƒ½ã€‚         |

è€ƒè™‘åˆ°ä½ å·²å…·å¤‡PyTorchå’ŒGPUé©±åŠ¨ç¯å¢ƒï¼Œ**æ¨èä½¿ç”¨PIPå®‰è£…**ä»¥å¿«é€Ÿä¸Šæ‰‹ã€‚

1.  **å®‰è£…ä¾èµ–**ï¼šç¡®ä¿ç³»ç»Ÿå…·å¤‡å¿…è¦çš„ç¼–è¯‘å·¥å…·å’Œåº“ã€‚
    ```bash
    sudo apt-get -y install libopenmpi-dev python3-pip
    ```
2.  **å®‰è£…TensorRT-LLM**ï¼šä½¿ç”¨pipä»NVIDIAå®˜æ–¹ç´¢å¼•å®‰è£…ã€‚
    ```bash
    pip3 install --upgrade pip setuptools
    pip3 install tensorrt_llm -U --extra-index-url https://pypi.nvidia.com
    ```
    å®‰è£…æˆåŠŸåï¼Œå¯ä»¥åœ¨ç»ˆç«¯ä¸­è¾“å…¥`pip list | grep tensorrt`æ¥ç¡®è®¤å®‰è£…ç‰ˆæœ¬ã€‚
3.  **éªŒè¯å®‰è£…**ï¼šåœ¨Pythonç¯å¢ƒä¸­å¯¼å…¥TensorRT-LLMåŒ…æ¥éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸã€‚
    ```python
    python3 -c "import tensorrt_llm; print(tensorrt_llm.__version__)"
    ```
    å¦‚æœèƒ½å¤ŸæˆåŠŸå¯¼å…¥å¹¶æ‰“å°å‡ºç‰ˆæœ¬å·ï¼Œåˆ™è¯´æ˜å®‰è£…æˆåŠŸã€‚

### ğŸš€ å¤§æ¨¡å‹éƒ¨ç½²ä¸æ¨ç†

TensorRT-LLMéƒ¨ç½²æ¨¡å‹çš„æ ¸å¿ƒæµç¨‹æ˜¯ï¼šå…ˆå°†Hugging Faceæ ¼å¼çš„æ¨¡å‹è½¬æ¢ä¸ºTensorRT-LLMæ ¼å¼ï¼Œç„¶åæ„å»ºä¼˜åŒ–åçš„æ¨ç†å¼•æ“ï¼Œæœ€åæ‰§è¡Œæ¨ç†ã€‚

```mermaid
flowchart TD
    A[Hugging Faceæ¨¡å‹] --> B[æ¨¡å‹è½¬æ¢]
    B --> C[æ„å»ºTRTå¼•æ“]
    C --> D{æ€§èƒ½æµ‹è¯•}
    D -- å‘½ä»¤è¡Œå¿«é€Ÿæµ‹è¯• --> E[run.pyè„šæœ¬]
    D -- åŸºå‡†æµ‹è¯• --> F[trtllm-benchå·¥å…·]
    D -- å¯åŠ¨APIæœåŠ¡ --> G[trtllm-serve]
    G --> H[HTTPè¯·æ±‚è°ƒç”¨]
```

æˆ‘ä»¬ä»¥ **Qwen1.5-4B-Chat** æ¨¡å‹ä¸ºä¾‹ï¼Œå±•ç¤ºéƒ¨ç½²çš„å…¨è¿‡ç¨‹ã€‚

1.  **è·å–æ¨¡å‹**
    ä»é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰ä¸‹è½½æ¨¡å‹ï¼š
    ```bash
    git lfs install
    git clone https://modelscope.cn/qwen/Qwen1.5-4B-Chat.git
    ```

2.  **æ¨¡å‹è½¬æ¢ä¸å¼•æ“æ„å»º**
    é¦–å…ˆéœ€è¦è·å–TensorRT-LLMçš„ç¤ºä¾‹ä»£ç ï¼š
    ```bash
    wget https://github.com/NVIDIA/TensorRT-LLM/archive/refs/tags/v0.10.0.tar.gz
    tar xvf v0.10.0.tar.gz
    cd TensorRT-LLM-0.10.0/examples/qwen
    ```
    å®‰è£…æ¨¡å‹ä¾èµ–å¹¶æ‰§è¡Œè½¬æ¢ï¼š
    ```bash
    pip install -r requirements.txt
    # å°†æ¨¡å‹è½¬æ¢ä¸ºTensorRT-LLMæ ¼å¼çš„æ£€æŸ¥ç‚¹
    python3 convert_checkpoint.py --model_dir /path/to/Qwen1.5-4B-Chat \
                                  --output_dir /path/to/trt_checkpoint \
                                  --dtype float16
    # æ„å»ºTensorRTæ¨ç†å¼•æ“
    trtllm-build --checkpoint_dir /path/to/trt_checkpoint \
                 --output_dir /path/to/trt_engines/qwen/1-gpu \
                 --gemm_plugin float16
    ```
    **å…³é”®å‚æ•°è¯´æ˜**ï¼š
    *   `--model_dir`: è¾“å…¥æ¨¡å‹è·¯å¾„ã€‚
    *   `--output_dir`: è½¬æ¢åæˆ–æ„å»ºå¼•æ“çš„è¾“å‡ºè·¯å¾„ã€‚
    *   `--dtype`: è®¡ç®—ç²¾åº¦ï¼Œ`float16` åœ¨A100ä¸Šèƒ½è¾ƒå¥½å¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦ã€‚
    *   `--gemm_plugin`: ä½¿ç”¨æ’ä»¶åŠ é€ŸçŸ©é˜µä¹˜æ³•ï¼Œå»ºè®®å¼€å¯ã€‚

3.  **æ‰§è¡Œæ¨ç†æµ‹è¯•**
    å¼•æ“æ„å»ºæˆåŠŸåï¼Œå¯ä»¥ä½¿ç”¨é™„å¸¦çš„`run.py`è„šæœ¬è¿›è¡Œå¿«é€Ÿæ¨ç†æµ‹è¯•ï¼š
    ```bash
    python3 ../run.py --input_text "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±" \
                      --max_output_len 500 \
                      --tokenizer_dir /path/to/Qwen1.5-4B-Chat \
                      --engine_dir /path/to/trt_engines/qwen/1-gpu
    ```

### ğŸ“Š æ€§èƒ½æµ‹è¯•ä¸åŸºå‡†æµ‹è¯•

ä¸ºäº†å…¨é¢è¯„ä¼°ä¼˜åŒ–åçš„æ¨¡å‹æ€§èƒ½ï¼ŒTensorRT-LLMæä¾›äº†ä¸“ä¸šçš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚

1.  **ä½¿ç”¨ `trtllm-bench` è¿›è¡ŒåŸºå‡†æµ‹è¯•**
    è¿™ä¸ªå·¥å…·å¯ä»¥è¯¦ç»†è¯„ä¼°æ¨¡å‹çš„ååé‡å’Œå»¶è¿ŸæŒ‡æ ‡ã€‚
    ```bash
    # é¦–å…ˆå‡†å¤‡ä¸€ä¸ªåŒ…å«æµ‹è¯•æç¤ºè¯çš„JSONLæ•°æ®é›†
    trtllm-bench throughput \
      --model /path/to/your/engine/directory \  # ä½¿ç”¨æ„å»ºå¥½çš„å¼•æ“ç›®å½•
      --dataset /path/to/dataset.jsonl \
      --tp 1 \          # å¼ é‡å¹¶è¡Œæ•°ï¼Œå•å¡è®¾ä¸º1
      --backend tensorrt \
      --report_json benchmark_results.json
    ```
    **å…³é”®æ€§èƒ½æŒ‡æ ‡è§£è¯»**ï¼š
    *   **Request Throughput (req/sec)**: æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°ã€‚
    *   **Total Output Throughput (tokens/sec)**: æ¯ç§’ç”Ÿæˆçš„ä»¤ç‰Œæ•°ï¼Œè¡¡é‡**ç”Ÿæˆé€Ÿåº¦**çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚
    *   **Time-to-First-Token (TTFT)**: ä»å‘é€è¯·æ±‚åˆ°æ”¶åˆ°ç¬¬ä¸€ä¸ªä»¤ç‰Œçš„æ—¶é—´ï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚
    *   **Time-Per-Output-Token (TPOT)**: å¹³å‡ç”Ÿæˆæ¯ä¸ªä»¤ç‰Œæ‰€éœ€æ—¶é—´ï¼Œä¸ç”Ÿæˆé€Ÿåº¦æˆåæ¯”ã€‚

2.  **å¯åŠ¨æ¨ç†APIæœåŠ¡**
    è‹¥è¦æä¾›ç±»ä¼¼OpenAIçš„APIæœåŠ¡ï¼Œå¯ä»¥ä½¿ç”¨`trtllm-serve`å‘½ä»¤ï¼š
    ```bash
    trtllm-serve /path/to/trt_engines/qwen/1-gpu \
                 --host localhost \
                 --port 8000 \
                 --max_batch_size 64
    ```
    æœåŠ¡å¯åŠ¨åï¼Œå³å¯é€šè¿‡HTTPè¯·æ±‚è°ƒç”¨ï¼š
    ```bash
    curl http://localhost:8000/v1/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "qwen",
        "prompt": "å¤ªé˜³ä¸ºä»€ä¹ˆä¸œå‡è¥¿è½ï¼Ÿ",
        "max_tokens": 500,
        "temperature": 0.8
      }'
    ```

### ğŸ’ å…³é”®æç¤ºä¸ä¼˜åŒ–æŠ€å·§

*   **ç²¾åº¦é€‰æ‹©**ï¼šåœ¨A100ä¸Šï¼Œ`float16` (FP16) å’Œ `bfloat16` (BF16) æ˜¯å¸¸ç”¨çš„æ¨ç†ç²¾åº¦ã€‚FP8æ˜¯æ›´æ–°çš„ä½ç²¾åº¦æ ¼å¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡ååé‡å¹¶é™ä½æ˜¾å­˜å ç”¨ï¼Œéƒ¨åˆ†æ¨¡å‹å·²æ”¯æŒã€‚
*   **æ’ä»¶å¯ç”¨**ï¼šåœ¨æ„å»ºå¼•æ“æ—¶ï¼Œ`--gemm_plugin` å’Œ `--gpt_attention_plugin` ç­‰æ’ä»¶èƒ½é€šè¿‡èåˆç®—å­æ¥æå‡æ€§èƒ½ï¼Œå»ºè®®å¯ç”¨ã€‚
*   **åˆ©ç”¨A100ç‰¹æ€§**ï¼šA100æ”¯æŒ**MIGï¼ˆå¤šå®ä¾‹GPUï¼‰** æŠ€æœ¯ï¼Œå¯ä»¥å°†å•å—80GB GPUåˆ’åˆ†ä¸ºå¤šä¸ªå°å‹GPUå®ä¾‹ï¼Œä»è€ŒåŒæ—¶æœåŠ¡å¤šä¸ªæ¨ç†ä»»åŠ¡ï¼Œæå‡èµ„æºåˆ©ç”¨ç‡ã€‚
*   **æ€§èƒ½è°ƒä¼˜**ï¼šåŸºå‡†æµ‹è¯•æ—¶ï¼Œé€šè¿‡è°ƒæ•´ `--concurrency` (å¹¶å‘è¯·æ±‚æ•°) ç­‰å‚æ•°ï¼Œå¯ä»¥æ¨¡æ‹Ÿä¸åŒè´Ÿè½½ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„ååé‡å’Œå»¶è¿Ÿå¹³è¡¡ç‚¹ã€‚



